# íŒ€1: QA ì‹œìŠ¤í…œ êµ¬í˜„ ê°€ì´ë“œ

**í”„ë¡œì íŠ¸**: EduMentor AI
**ë‹´ë‹¹**: íŒ€1 (2ëª…)
**ê¸°ê°„**: 2ì£¼
**ëª©í‘œ**: í•™ìŠµìë£Œ ê¸°ë°˜ 1-2ì´ˆ ì‘ë‹µ QA ì‹œìŠ¤í…œ

---

## ğŸ“‹ íŒ€1 ì—­í•  ë¶„ë‹´

| ê°œë°œì       | ë‹´ë‹¹ ê¸°ëŠ¥                                    | ì˜ˆìƒ ì‹œê°„ |
| --------- | ---------------------------------------- | ----- |
| **ê°œë°œì A** | ìë£Œ ì—…ë¡œë“œ, Upstage Parse, ì„ë² ë”©, ChromaDB ì €ì¥  | 5ì¼    |
| **ê°œë°œì B** | QA RAG íŒŒì´í”„ë¼ì¸, LangGraph Agent, ì‘ë‹µ ì†ë„ ìµœì í™” | 5ì¼    |

---

## ğŸ¯ í•µì‹¬ ìš”êµ¬ì‚¬í•­

| ìš”êµ¬ì‚¬í•­ | ëª©í‘œ | ê²€ì¦ ë°©ë²• |
|----------|------|-----------|
| **ì‘ë‹µ ì†ë„** | Retrieve í¬í•¨ 1-2ì´ˆ | ë¡œê¹…ìœ¼ë¡œ ì¸¡ì • |
| **LangChain** | RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶• | ì½”ë“œ ë¦¬ë·° |
| **Upstage API** | Parse, Embeddings, Chat ì‚¬ìš© | API í˜¸ì¶œ ë¡œê·¸ |
| **ChromaDB** | ë²¡í„° DB êµ¬ì¶• | Collection í™•ì¸ |
| **LangGraph** | Agent ì›Œí¬í”Œë¡œìš° | ë‹¤ì´ì–´ê·¸ë¨ ì‘ì„± |

---

## Week 1: ê¸°ë°˜ êµ¬ì¶•

### Day 1: í™˜ê²½ ì„¤ì • ë° ChromaDB ì—°ë™

#### ê°œë°œì A + B (ê³µë™ ì‘ì—…)

**1.1 Python í”„ë¡œì íŠ¸ ìƒì„±**

```bash
# í”„ë¡œì íŠ¸ êµ¬ì¡°
team1_qa/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ config.py           # ì„¤ì •
â”œâ”€â”€ models.py           # Pydantic ëª¨ë¸
â”œâ”€â”€ workflow.py         # LangGraph ì›Œí¬í”Œë¡œìš°
â”œâ”€â”€ api.py              # FastAPI ì—”ë“œí¬ì¸íŠ¸
â”œâ”€â”€ parsers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pdf_parser.py   # PDF íŒŒì‹±
â”‚   â””â”€â”€ ppt_parser.py   # PPT íŒŒì‹±
â””â”€â”€ utils/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ upstage_client.py
    â””â”€â”€ chroma_client.py
```

**1.2 ì˜ì¡´ì„± ì„¤ì¹˜**

```bash
# requirements.txt
langchain==0.1.0
langgraph==0.0.40
langsmith==0.0.77
upstage==0.1.0
chromadb==0.4.22
fastapi==0.109.0
uvicorn[standard]==0.27.0
python-multipart==0.0.6
PyMuPDF==1.23.8
python-pptx==0.6.21
python-dotenv==1.0.0
aiohttp==3.9.1
```

```bash
pip install -r requirements.txt
```

**1.3 í™˜ê²½ ë³€ìˆ˜ ì„¤ì •**

```python
# config.py
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    # Upstage API
    UPSTAGE_API_KEY: str

    # ChromaDB
    CHROMA_HOST: str = "localhost"
    CHROMA_PORT: int = 8001

    # File Storage
    UPLOAD_DIR: str = "./uploads"

    class Config:
        env_file = ".env"

settings = Settings()
```

```bash
# .env
UPSTAGE_API_KEY=your_upstage_api_key_here
CHROMA_HOST=localhost
CHROMA_PORT=8001
```

**1.4 ChromaDB í´ë¼ì´ì–¸íŠ¸ ì‘ì„±**

```python
# utils/chroma_client.py
import chromadb
from chromadb.config import Settings as ChromaSettings
from typing import List, Dict
from config import settings

class ChromaClient:
    def __init__(self):
        self.client = chromadb.HttpClient(
            host=settings.CHROMA_HOST,
            port=settings.CHROMA_PORT,
            settings=ChromaSettings(
                anonymized_telemetry=False
            )
        )

    def get_or_create_collection(self, name: str):
        """ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°"""
        return self.client.get_or_create_collection(
            name=name,
            metadata={"hnsw:space": "cosine"}  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        )

    def add_documents(
        self,
        collection_name: str,
        documents: List[str],
        metadatas: List[Dict],
        ids: List[str]
    ):
        """ë¬¸ì„œ ì¶”ê°€"""
        collection = self.get_or_create_collection(collection_name)
        collection.add(
            documents=documents,
            metadatas=metadatas,
            ids=ids
        )

    def search(
        self,
        collection_name: str,
        query_texts: List[str],
        n_results: int = 3,
        filter_dict: Dict = None
    ):
        """ìœ ì‚¬ë„ ê²€ìƒ‰"""
        collection = self.get_or_create_collection(collection_name)
        return collection.query(
            query_texts=query_texts,
            n_results=n_results,
            where=filter_dict
        )

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
chroma_client = ChromaClient()
```

**1.5 ChromaDB í…ŒìŠ¤íŠ¸**

```python
# test_chroma.py
from utils.chroma_client import chroma_client

def test_chromadb():
    # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì¶”ê°€
    chroma_client.add_documents(
        collection_name="test",
        documents=["JPAëŠ” Java Persistence APIì…ë‹ˆë‹¤."],
        metadatas=[{"source": "test"}],
        ids=["test_1"]
    )

    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    results = chroma_client.search(
        collection_name="test",
        query_texts=["JPAë€?"],
        n_results=1
    )

    print("âœ… ChromaDB ì—°ë™ ì„±ê³µ!")
    print(f"ê²€ìƒ‰ ê²°ê³¼: {results['documents'][0][0]}")

if __name__ == "__main__":
    test_chromadb()
```

```bash
# ì‹¤í–‰
python test_chroma.py
```

---

### Day 2-3: ìë£Œ ì—…ë¡œë“œ ë° Upstage Parse (ê°œë°œì A)

**2.1 Upstage Client ì‘ì„±**

```python
# utils/upstage_client.py
from upstage import Upstage
from config import settings
import asyncio

class UpstageClient:
    def __init__(self):
        self.client = Upstage(api_key=settings.UPSTAGE_API_KEY)

    async def parse_pdf(self, file_path: str) -> dict:
        """PDF íŒŒì‹±"""
        result = await asyncio.to_thread(
            self.client.document_parse,
            file=file_path
        )
        return result

    async def embed_text(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ ì„ë² ë”©"""
        from langchain_upstage import UpstageEmbeddings

        embeddings = UpstageEmbeddings(
            api_key=settings.UPSTAGE_API_KEY,
            model="embedding-query"
        )

        return await embeddings.aembed_query(text)

    async def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ì„ë² ë”©"""
        from langchain_upstage import UpstageEmbeddings

        embeddings = UpstageEmbeddings(
            api_key=settings.UPSTAGE_API_KEY,
            model="embedding-passage"
        )

        return await embeddings.aembed_documents(texts)

upstage_client = UpstageClient()
```

**2.2 PDF íŒŒì„œ ì‘ì„±**

```python
# parsers/pdf_parser.py
from typing import List, Dict
from utils.upstage_client import upstage_client
import logging

logger = logging.getLogger(__name__)

class PDFParser:
    async def parse(self, file_path: str) -> List[Dict]:
        """PDF íŒŒì‹± ë° ìš”ì†Œë³„ ë¶„ë¦¬"""
        logger.info(f"Parsing PDF: {file_path}")

        # Upstage Document Parse
        parsed = await upstage_client.parse_pdf(file_path)

        content_blocks = []

        # elements ìˆœíšŒ
        for element in parsed.get('elements', []):
            element_type = element.get('type')

            if element_type == 'text':
                content_blocks.append({
                    'type': 'text',
                    'content': element.get('content', ''),
                    'page': element.get('page', 1),
                    'category': element.get('category', 'paragraph')
                })

            elif element_type == 'table':
                # í‘œëŠ” í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                table_text = self._table_to_text(element.get('content'))
                content_blocks.append({
                    'type': 'table',
                    'content': table_text,
                    'page': element.get('page', 1)
                })

        logger.info(f"Parsed {len(content_blocks)} blocks from PDF")
        return content_blocks

    def _table_to_text(self, table_data) -> str:
        """í‘œ ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        # ê°„ë‹¨í•œ ë³€í™˜ ë¡œì§
        return str(table_data)

pdf_parser = PDFParser()
```

**2.3 PPT íŒŒì„œ ì‘ì„± (ì„ íƒì‚¬í•­)**

```python
# parsers/ppt_parser.py
from pptx import Presentation
from typing import List, Dict
import logging

logger = logging.getLogger(__name__)

class PPTParser:
    def parse(self, file_path: str) -> List[Dict]:
        """PPT íŒŒì‹±"""
        logger.info(f"Parsing PPT: {file_path}")

        prs = Presentation(file_path)
        content_blocks = []

        for slide_idx, slide in enumerate(prs.slides, 1):
            # ìŠ¬ë¼ì´ë“œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            slide_text = []

            for shape in slide.shapes:
                if hasattr(shape, "text"):
                    slide_text.append(shape.text)

            if slide_text:
                content_blocks.append({
                    'type': 'slide',
                    'content': '\n'.join(slide_text),
                    'page': slide_idx
                })

        logger.info(f"Parsed {len(content_blocks)} slides from PPT")
        return content_blocks

ppt_parser = PPTParser()
```

**2.4 ìë£Œ ì—…ë¡œë“œ ì›Œí¬í”Œë¡œìš°**

```python
# workflow.py (Part 1: Upload)
from langgraph.graph import StateGraph, START, END
from typing import TypedDict, List, Dict
from parsers.pdf_parser import pdf_parser
from parsers.ppt_parser import ppt_parser
from utils.upstage_client import upstage_client
from utils.chroma_client import chroma_client
import logging

logger = logging.getLogger(__name__)

class UploadState(TypedDict):
    material_id: int
    file_path: str
    file_type: str  # "pdf" or "ppt"
    parsed_blocks: List[Dict]
    embeddings: List[List[float]]
    status: str

async def parse_document_node(state: UploadState) -> dict:
    """1ë‹¨ê³„: ë¬¸ì„œ íŒŒì‹±"""
    file_path = state["file_path"]
    file_type = state["file_type"]

    logger.info(f"Parsing {file_type}: {file_path}")

    if file_type == "pdf":
        parsed_blocks = await pdf_parser.parse(file_path)
    elif file_type == "ppt":
        parsed_blocks = ppt_parser.parse(file_path)
    else:
        raise ValueError(f"Unsupported file type: {file_type}")

    return {"parsed_blocks": parsed_blocks}

async def embed_and_store_node(state: UploadState) -> dict:
    """2ë‹¨ê³„: ì„ë² ë”© ë° ChromaDB ì €ì¥"""
    material_id = state["material_id"]
    parsed_blocks = state["parsed_blocks"]

    logger.info(f"Embedding {len(parsed_blocks)} blocks")

    # ë°°ì¹˜ ì„ë² ë”© (íš¨ìœ¨ì„±)
    texts = [block['content'] for block in parsed_blocks]
    embeddings = await upstage_client.embed_documents(texts)

    logger.info("Storing in ChromaDB")

    # ChromaDBì— ì €ì¥
    documents = []
    metadatas = []
    ids = []

    for idx, block in enumerate(parsed_blocks):
        documents.append(block['content'])
        metadatas.append({
            'material_id': material_id,
            'page': block['page'],
            'type': block['type']
        })
        ids.append(f"material_{material_id}_block_{idx}")

    chroma_client.add_documents(
        collection_name="learning_materials",
        documents=documents,
        metadatas=metadatas,
        ids=ids
    )

    return {
        "embeddings": embeddings,
        "status": "completed"
    }

# ì—…ë¡œë“œ ì›Œí¬í”Œë¡œìš° ìƒì„±
def create_upload_workflow():
    graph = StateGraph(UploadState)

    graph.add_node("parse", parse_document_node)
    graph.add_node("embed_store", embed_and_store_node)

    graph.add_edge(START, "parse")
    graph.add_edge("parse", "embed_store")
    graph.add_edge("embed_store", END)

    return graph.compile()

upload_workflow = create_upload_workflow()
```

**2.5 ì—…ë¡œë“œ API ì—”ë“œí¬ì¸íŠ¸**

```python
# api.py (Part 1: Upload)
from fastapi import APIRouter, UploadFile, HTTPException
from pydantic import BaseModel
import shutil
import os
from workflow import upload_workflow
from config import settings

router = APIRouter(prefix="/materials", tags=["Materials"])

class UploadResponse(BaseModel):
    material_id: int
    status: str
    blocks_count: int

@router.post("/upload", response_model=UploadResponse)
async def upload_material(
    file: UploadFile,
    material_id: int
):
    """í•™ìŠµìë£Œ ì—…ë¡œë“œ ë° íŒŒì‹±"""

    # íŒŒì¼ íƒ€ì… í™•ì¸
    file_ext = file.filename.split('.')[-1].lower()
    if file_ext not in ['pdf', 'ppt', 'pptx']:
        raise HTTPException(
            status_code=400,
            detail="Only PDF and PPT files are supported"
        )

    # íŒŒì¼ ì €ì¥
    os.makedirs(settings.UPLOAD_DIR, exist_ok=True)
    file_path = os.path.join(settings.UPLOAD_DIR, file.filename)

    with open(file_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)

    # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    result = await upload_workflow.ainvoke({
        "material_id": material_id,
        "file_path": file_path,
        "file_type": "pdf" if file_ext == "pdf" else "ppt"
    })

    return UploadResponse(
        material_id=material_id,
        status=result["status"],
        blocks_count=len(result["parsed_blocks"])
    )
```

---

### Day 4-5: QA RAG íŒŒì´í”„ë¼ì¸ (ê°œë°œì B)

**3.1 QA State ì •ì˜**

```python
# workflow.py (Part 2: QA)
class QAState(TypedDict):
    question: str
    material_id: int
    retrieved_docs: List[Dict]
    answer: str
    sources: List[Dict]
    response_time: float
```

**3.2 Retrieve ë…¸ë“œ (0.2-0.3ì´ˆ ëª©í‘œ)**

```python
# workflow.py (Part 2: QA - Retrieve)
import time

async def retrieve_node(state: QAState) -> dict:
    """ChromaDBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"""
    start_time = time.time()

    question = state["question"]
    material_id = state["material_id"]

    logger.info(f"Retrieving docs for: {question}")

    # ê²€ìƒ‰ (k=3ìœ¼ë¡œ ì œí•œ - ì†ë„ ìµœì í™”)
    results = chroma_client.search(
        collection_name="learning_materials",
        query_texts=[question],
        n_results=3,
        filter_dict={"material_id": material_id}
    )

    # ê²°ê³¼ êµ¬ì„±
    retrieved_docs = []
    for i in range(len(results['documents'][0])):
        retrieved_docs.append({
            'content': results['documents'][0][i],
            'page': results['metadatas'][0][i]['page'],
            'type': results['metadatas'][0][i]['type'],
            'distance': results['distances'][0][i]
        })

    retrieve_time = time.time() - start_time
    logger.info(f"âš¡ Retrieve time: {retrieve_time:.3f}s")

    return {"retrieved_docs": retrieved_docs}
```

**3.3 Generate ë…¸ë“œ (0.8-1.0ì´ˆ ëª©í‘œ)**

```python
# workflow.py (Part 2: QA - Generate)
from langchain_upstage import ChatUpstage
from langchain.schema import HumanMessage

async def generate_answer_node(state: QAState) -> dict:
    """Upstage Solarë¡œ ë‹µë³€ ìƒì„±"""
    start_time = time.time()

    question = state["question"]
    retrieved_docs = state["retrieved_docs"]

    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_parts = []
    for doc in retrieved_docs:
        context_parts.append(
            f"[í˜ì´ì§€ {doc['page']}]\n{doc['content']}"
        )
    context = "\n\n---\n\n".join(context_parts)

    # Upstage Solar Mini (ë¹ ë¥¸ ì‘ë‹µ)
    llm = ChatUpstage(
        api_key=settings.UPSTAGE_API_KEY,
        model="solar-1-mini-chat",
        temperature=0.3
    )

    prompt = f"""ë‹¹ì‹ ì€ í•™ìŠµìë£Œ ê¸°ë°˜ QA ë´‡ì…ë‹ˆë‹¤.

**í•™ìŠµìë£Œ ë‚´ìš©**:
{context}

**í•™ìƒ ì§ˆë¬¸**: {question}

**ë‹µë³€ ê·œì¹™**:
1. í•™ìŠµìë£Œì— ìˆëŠ” ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
2. ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš” (3-5ë¬¸ì¥)
3. ê´€ë ¨ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ëª…ì‹œí•˜ì„¸ìš”

ë‹µë³€:"""

    response = await llm.ainvoke([HumanMessage(content=prompt)])
    answer = response.content

    # ì¶œì²˜ ì •ë³´
    sources = [
        {
            "page": doc["page"],
            "excerpt": doc["content"][:100] + "..."
        }
        for doc in retrieved_docs
    ]

    generate_time = time.time() - start_time
    logger.info(f"âš¡ Generate time: {generate_time:.3f}s")

    return {
        "answer": answer,
        "sources": sources
    }
```

**3.4 QA ì›Œí¬í”Œë¡œìš° ì™„ì„±**

```python
# workflow.py (Part 2: QA - Complete)
def create_qa_workflow():
    """QA LangGraph ì›Œí¬í”Œë¡œìš°"""
    graph = StateGraph(QAState)

    # ë…¸ë“œ ì¶”ê°€
    graph.add_node("retrieve", retrieve_node)
    graph.add_node("generate", generate_answer_node)

    # ì—£ì§€ (ìˆœì°¨ ì‹¤í–‰)
    graph.add_edge(START, "retrieve")
    graph.add_edge("retrieve", "generate")
    graph.add_edge("generate", END)

    return graph.compile()

qa_workflow = create_qa_workflow()
```

**3.5 QA API ì—”ë“œí¬ì¸íŠ¸**

```python
# api.py (Part 2: QA)
import time

class QARequest(BaseModel):
    material_id: int
    question: str

class QAResponse(BaseModel):
    answer: str
    sources: List[Dict]
    response_time_ms: int

@router.post("/ask", response_model=QAResponse)
async def ask_question(request: QARequest):
    """í•™ìŠµìë£Œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ"""
    start_time = time.time()

    # LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    result = await qa_workflow.ainvoke({
        "question": request.question,
        "material_id": request.material_id
    })

    response_time = int((time.time() - start_time) * 1000)

    # 2ì´ˆ ì´ˆê³¼ ì‹œ ê²½ê³ 
    if response_time > 2000:
        logger.warning(f"âš ï¸ Slow response: {response_time}ms")
    else:
        logger.info(f"âœ… Response time: {response_time}ms")

    return QAResponse(
        answer=result["answer"],
        sources=result["sources"],
        response_time_ms=response_time
    )
```

---

## Week 2: ìµœì í™” ë° ì™„ì„±

### Day 6-7: LangGraph Agent ê³ ë„í™”

**4.1 ì‘ë‹µ ìºì‹± ì¶”ê°€**

```python
# utils/cache.py
from functools import lru_cache
from typing import List

class QueryCache:
    def __init__(self, maxsize=100):
        self.cache = {}
        self.maxsize = maxsize

    def get(self, key: str):
        return self.cache.get(key)

    def set(self, key: str, value):
        if len(self.cache) >= self.maxsize:
            # FIFO ì œê±°
            self.cache.pop(next(iter(self.cache)))
        self.cache[key] = value

query_cache = QueryCache()
```

**4.2 ìºì‹± ì ìš©**

```python
# workflow.py (ìºì‹± ì¶”ê°€)
async def retrieve_node(state: QAState) -> dict:
    question = state["question"]
    material_id = state["material_id"]

    # ìºì‹œ í‚¤ ìƒì„±
    cache_key = f"{material_id}:{question}"

    # ìºì‹œ í™•ì¸
    cached = query_cache.get(cache_key)
    if cached:
        logger.info("âœ… Cache hit!")
        return {"retrieved_docs": cached}

    # ìºì‹œ ë¯¸ìŠ¤ - ê²€ìƒ‰ ìˆ˜í–‰
    results = chroma_client.search(...)

    retrieved_docs = [...]

    # ìºì‹œ ì €ì¥
    query_cache.set(cache_key, retrieved_docs)

    return {"retrieved_docs": retrieved_docs}
```

**4.3 ë©€í‹° ì¿¼ë¦¬ ê²€ìƒ‰ (ì •í™•ë„ í–¥ìƒ)**

```python
# workflow.py (ë©€í‹° ì¿¼ë¦¬)
from langchain_upstage import ChatUpstage

async def generate_multi_queries(question: str) -> List[str]:
    """ì§ˆë¬¸ì„ ë‹¤ì–‘í•œ í˜•íƒœë¡œ ë³€í™˜"""
    llm = ChatUpstage(
        api_key=settings.UPSTAGE_API_KEY,
        model="solar-1-mini-chat"
    )

    prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì„ 3ê°€ì§€ ë‹¤ë¥¸ í˜•íƒœë¡œ ë³€í™˜í•˜ì„¸ìš”:

ì§ˆë¬¸: {question}

ë³€í™˜ëœ ì§ˆë¬¸ë“¤ (ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„):"""

    response = await llm.ainvoke([HumanMessage(content=prompt)])
    queries = response.content.strip().split('\n')

    return [question] + queries[:2]  # ì›ë³¸ + 2ê°œ ë³€í™˜

async def retrieve_node(state: QAState) -> dict:
    question = state["question"]
    material_id = state["material_id"]

    # ë©€í‹° ì¿¼ë¦¬ ìƒì„±
    queries = await generate_multi_queries(question)

    all_docs = []
    seen_ids = set()

    # ê° ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
    for query in queries:
        results = chroma_client.search(
            collection_name="learning_materials",
            query_texts=[query],
            n_results=2,
            filter_dict={"material_id": material_id}
        )

        # ì¤‘ë³µ ì œê±°
        for i, doc_id in enumerate(results['ids'][0]):
            if doc_id not in seen_ids:
                seen_ids.add(doc_id)
                all_docs.append({
                    'content': results['documents'][0][i],
                    'page': results['metadatas'][0][i]['page'],
                    'distance': results['distances'][0][i]
                })

    # ê±°ë¦¬ ê¸°ì¤€ ì •ë ¬ (ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ 3ê°œ)
    all_docs.sort(key=lambda x: x['distance'])
    retrieved_docs = all_docs[:3]

    return {"retrieved_docs": retrieved_docs}
```

---

### Day 8-9: ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ìµœì í™”

**5.1 ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸**

```python
# tests/test_performance.py
import asyncio
import time
from workflow import qa_workflow

async def test_qa_performance():
    """QA ì‘ë‹µ ì†ë„ í…ŒìŠ¤íŠ¸"""

    test_cases = [
        {
            "question": "JPA Entityë€ ë¬´ì—‡ì¸ê°€ìš”?",
            "material_id": 1
        },
        {
            "question": "@Transactional ì–´ë…¸í…Œì´ì…˜ì€ ì–¸ì œ ì‚¬ìš©í•˜ë‚˜ìš”?",
            "material_id": 1
        },
        {
            "question": "Spring Bootì˜ Auto Configurationì€ ì–´ë–»ê²Œ ë™ì‘í•˜ë‚˜ìš”?",
            "material_id": 1
        }
    ]

    results = []

    for test in test_cases:
        print(f"\nì§ˆë¬¸: {test['question']}")

        start = time.time()

        result = await qa_workflow.ainvoke({
            "question": test["question"],
            "material_id": test["material_id"]
        })

        elapsed = time.time() - start

        status = "âœ… PASS" if elapsed < 2.0 else "âŒ FAIL"
        print(f"ì‘ë‹µ ì‹œê°„: {elapsed:.3f}s - {status}")
        print(f"ë‹µë³€: {result['answer'][:100]}...")

        results.append({
            "question": test["question"],
            "time": elapsed,
            "passed": elapsed < 2.0
        })

    # í†µê³„
    total = len(results)
    passed = sum(1 for r in results if r['passed'])
    avg_time = sum(r['time'] for r in results) / total

    print("\n" + "="*50)
    print(f"ì´ í…ŒìŠ¤íŠ¸: {total}")
    print(f"í†µê³¼: {passed}/{total}")
    print(f"í‰ê·  ì‘ë‹µ ì‹œê°„: {avg_time:.3f}s")
    print("="*50)

if __name__ == "__main__":
    asyncio.run(test_qa_performance())
```

```bash
# ì‹¤í–‰
python tests/test_performance.py
```

**5.2 ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§**

```python
# utils/profiler.py
import time
from functools import wraps

def profile(func):
    """í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ë°ì½”ë ˆì´í„°"""
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start = time.time()
        result = await func(*args, **kwargs)
        elapsed = time.time() - start

        print(f"â±ï¸ {func.__name__}: {elapsed:.3f}s")

        return result
    return wrapper

# ì‚¬ìš© ì˜ˆì‹œ
@profile
async def retrieve_node(state: QAState) -> dict:
    ...
```

---

### Day 10: ë¬¸ì„œí™” ë° ë°°í¬ ì¤€ë¹„

**6.1 API ë¬¸ì„œí™” (FastAPI ìë™ ìƒì„±)**

```python
# main.py
from fastapi import FastAPI
from api import router
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

app = FastAPI(
    title="EduMentor QA API",
    description="í•™ìŠµìë£Œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ",
    version="1.0.0"
)

app.include_router(router)

@app.get("/health")
async def health_check():
    return {"status": "healthy"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

```bash
# ì„œë²„ ì‹¤í–‰
python main.py
```

API ë¬¸ì„œ ìë™ ìƒì„±: http://localhost:8000/docs

**6.2 README ì‘ì„±**

```markdown
# íŒ€1: QA ì‹œìŠ¤í…œ

## ì„¤ì¹˜

```bash
pip install -r requirements.txt
```

## ì‹¤í–‰

```bash
# ChromaDB ì‹œì‘
docker run -p 8001:8000 chromadb/chroma:latest

# FastAPI ì„œë²„ ì‹œì‘
python main.py
```

## API ì‚¬ìš©ë²•

### 1. í•™ìŠµìë£Œ ì—…ë¡œë“œ
```bash
curl -X POST http://localhost:8000/materials/upload \
  -F "file=@spring_guide.pdf" \
  -F "material_id=1"
```

### 2. ì§ˆë¬¸í•˜ê¸°
```bash
curl -X POST http://localhost:8000/materials/ask \
  -H "Content-Type: application/json" \
  -d '{
    "material_id": 1,
    "question": "JPA Entityë€ ë¬´ì—‡ì¸ê°€ìš”?"
  }'
```

## ì„±ëŠ¥ ëª©í‘œ

- **ì‘ë‹µ ì†ë„**: 1-2ì´ˆ (Retrieve í¬í•¨)
- **ì •í™•ë„**: í•™ìŠµìë£Œ ê¸°ë°˜ ì •í™•í•œ ë‹µë³€
- **ì¶œì²˜ í‘œì‹œ**: í˜ì´ì§€ ë²ˆí˜¸ ëª…ì‹œ

---

## ğŸ“Š íŒ€1 ì™„ì„± ì²´í¬ë¦¬ìŠ¤íŠ¸

### í•µì‹¬ ê¸°ëŠ¥
- [ ] Upstage Document Parse ì—°ë™
- [ ] Upstage Embeddings ì—°ë™
- [ ] ChromaDB ë²¡í„° ì €ì¥
- [ ] LangGraph QA ì›Œí¬í”Œë¡œìš°
- [ ] 1-2ì´ˆ ì‘ë‹µ ì†ë„ ë‹¬ì„±
- [ ] ì¶œì²˜ í‘œì‹œ ê¸°ëŠ¥

### API ì—”ë“œí¬ì¸íŠ¸
- [ ] POST `/materials/upload` - ìë£Œ ì—…ë¡œë“œ
- [ ] POST `/materials/ask` - ì§ˆì˜ì‘ë‹µ

### í…ŒìŠ¤íŠ¸
- [ ] ì—…ë¡œë“œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
- [ ] QA ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (1-2ì´ˆ)
- [ ] ë‹¤ì–‘í•œ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸

### ë¬¸ì„œí™”
- [ ] API ë¬¸ì„œ (FastAPI Swagger)
- [ ] README.md
- [ ] ì½”ë“œ ì£¼ì„

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

íŒ€1 ì™„ë£Œ í›„:
1. **íŒ€2ì™€ í†µí•©**: Spring Boot ì—°ë™
2. **ì „ì²´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸**: E2E í…ŒìŠ¤íŠ¸
3. **ì„±ëŠ¥ ìµœì í™”**: ë³‘ëª© êµ¬ê°„ ê°œì„ 
4. **ë°°í¬ ì¤€ë¹„**: Docker ì´ë¯¸ì§€ ìƒì„±

---

**ì‘ì„±ì¼**: 2025-10-28
**ë‹´ë‹¹**: íŒ€1 (ê°œë°œì A, B)
